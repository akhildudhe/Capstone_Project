{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import resample\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.optimizers import RMSprop, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 595212 entries, 0 to 595211\n",
      "Data columns (total 59 columns):\n",
      "id                595212 non-null int64\n",
      "target            595212 non-null int64\n",
      "ps_ind_01         595212 non-null int64\n",
      "ps_ind_02_cat     595212 non-null int64\n",
      "ps_ind_03         595212 non-null int64\n",
      "ps_ind_04_cat     595212 non-null int64\n",
      "ps_ind_05_cat     595212 non-null int64\n",
      "ps_ind_06_bin     595212 non-null int64\n",
      "ps_ind_07_bin     595212 non-null int64\n",
      "ps_ind_08_bin     595212 non-null int64\n",
      "ps_ind_09_bin     595212 non-null int64\n",
      "ps_ind_10_bin     595212 non-null int64\n",
      "ps_ind_11_bin     595212 non-null int64\n",
      "ps_ind_12_bin     595212 non-null int64\n",
      "ps_ind_13_bin     595212 non-null int64\n",
      "ps_ind_14         595212 non-null int64\n",
      "ps_ind_15         595212 non-null int64\n",
      "ps_ind_16_bin     595212 non-null int64\n",
      "ps_ind_17_bin     595212 non-null int64\n",
      "ps_ind_18_bin     595212 non-null int64\n",
      "ps_reg_01         595212 non-null float64\n",
      "ps_reg_02         595212 non-null float64\n",
      "ps_reg_03         595212 non-null float64\n",
      "ps_car_01_cat     595212 non-null int64\n",
      "ps_car_02_cat     595212 non-null int64\n",
      "ps_car_03_cat     595212 non-null int64\n",
      "ps_car_04_cat     595212 non-null int64\n",
      "ps_car_05_cat     595212 non-null int64\n",
      "ps_car_06_cat     595212 non-null int64\n",
      "ps_car_07_cat     595212 non-null int64\n",
      "ps_car_08_cat     595212 non-null int64\n",
      "ps_car_09_cat     595212 non-null int64\n",
      "ps_car_10_cat     595212 non-null int64\n",
      "ps_car_11_cat     595212 non-null int64\n",
      "ps_car_11         595212 non-null int64\n",
      "ps_car_12         595212 non-null float64\n",
      "ps_car_13         595212 non-null float64\n",
      "ps_car_14         595212 non-null float64\n",
      "ps_car_15         595212 non-null float64\n",
      "ps_calc_01        595212 non-null float64\n",
      "ps_calc_02        595212 non-null float64\n",
      "ps_calc_03        595212 non-null float64\n",
      "ps_calc_04        595212 non-null int64\n",
      "ps_calc_05        595212 non-null int64\n",
      "ps_calc_06        595212 non-null int64\n",
      "ps_calc_07        595212 non-null int64\n",
      "ps_calc_08        595212 non-null int64\n",
      "ps_calc_09        595212 non-null int64\n",
      "ps_calc_10        595212 non-null int64\n",
      "ps_calc_11        595212 non-null int64\n",
      "ps_calc_12        595212 non-null int64\n",
      "ps_calc_13        595212 non-null int64\n",
      "ps_calc_14        595212 non-null int64\n",
      "ps_calc_15_bin    595212 non-null int64\n",
      "ps_calc_16_bin    595212 non-null int64\n",
      "ps_calc_17_bin    595212 non-null int64\n",
      "ps_calc_18_bin    595212 non-null int64\n",
      "ps_calc_19_bin    595212 non-null int64\n",
      "ps_calc_20_bin    595212 non-null int64\n",
      "dtypes: float64(10), int64(49)\n",
      "memory usage: 267.9 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"train(1).csv\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "colmns=['Model','DownSample','Normalise','Accuracy','Precision','Recall','F1-Score']\n",
    "final_result_df=pd.DataFrame(columns=colmns)\n",
    "Index_count=0\n",
    "def df_split(dataframe):\n",
    "    model_df=dataframe[:]\n",
    "    model_df.drop(['id','target'],inplace=True,axis=1)\n",
    "    colmns=model_df.columns\n",
    "    train,test=train_test_split(dataframe,random_state=7,test_size=0.7)\n",
    "    train_y=train['target']\n",
    "    train_x=train[colmns]\n",
    "    test_y=test['target']\n",
    "    test_x=test[colmns]\n",
    "    return train_x,train_y,test_x,test_y\n",
    "\n",
    "def downsample(one_samp_percent):\n",
    "    tot=int(len(df[df['target']==1])/one_samp_percent*100)\n",
    "    zero_samp_percent=tot-int(len(df[df['target']==1]))\n",
    "    df_zero=df[df['target']==0]\n",
    "    df_majority_downsampled = resample(df_zero, replace=False,n_samples=zero_samp_percent,random_state=123)\n",
    "    df_downsample=pd.concat([df_majority_downsampled,df[df['target']==1]])\n",
    "    return df_downsample\n",
    "\n",
    "\n",
    "def print_measures(model_name,m_name,DwnSam,norm,Test_Y,Pred_Y):\n",
    "    global Index_count,final_result_df\n",
    "    p,q,r=m_name,Test_Y,Pred_Y\n",
    "    s=metrics.accuracy_score(q,r)\n",
    "    v=metrics.recall_score(q,r)\n",
    "    u=metrics.precision_score(q,r)\n",
    "    t=metrics.f1_score(q,r)\n",
    "    print('The accuracy by %s Model is : %f '%(model_name,s))\n",
    "    cnf=metrics.confusion_matrix(q, r)\n",
    "    print('The confusion matrix for the model is : \\n',cnf)\n",
    "    print('The F1 score for this model is : ',t)\n",
    "    print('The Precision score for this model is : ',u)\n",
    "    print('The Recall score for this model is : ',v)\n",
    "    row=pd.DataFrame([[m_name,DwnSam,norm,s,u,v,t]],columns=colmns, index=[Index_count])\n",
    "    final_result_df=final_result_df.append(row)\n",
    "    Index_count+=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logitic Regression on Actual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\pandas\\core\\frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy by Logistic Regression on Actual Data Model is : 0.963581 \n",
      "The confusion matrix for the model is : \n",
      " [[401475      0]\n",
      " [ 15174      0]]\n",
      "The F1 score for this model is :  0.0\n",
      "The Precision score for this model is :  0.0\n",
      "The Recall score for this model is :  0.0\n"
     ]
    }
   ],
   "source": [
    "train_x,train_y,test_x,test_y=df_split(df)\n",
    "lr=LogisticRegression()\n",
    "lr_model=lr.fit(train_x,train_y)\n",
    "pred_y=lr_model.predict(test_x)\n",
    "\n",
    "print_measures('Logistic Regression on Actual Data','Logistic Regression','No','No',test_y,pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logitic Regression on Downsampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.5\n",
      "0    0.5\n",
      "Name: target, dtype: float64\n",
      "43388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\pandas\\core\\frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy by Logistic Regression with downsampling Model is : 0.584025 \n",
      "The confusion matrix for the model is : \n",
      " [[9348 5854]\n",
      " [6780 8390]]\n",
      "The F1 score for this model is :  0.570476643775073\n",
      "The Precision score for this model is :  0.5890199382196012\n",
      "The Recall score for this model is :  0.5530652603823335\n"
     ]
    }
   ],
   "source": [
    "downsample_by_zero=downsample(50)\n",
    "print(downsample_by_zero['target'].value_counts(normalize=True))\n",
    "print(len(downsample_by_zero))\n",
    "\n",
    "train_ds_x,train_ds_y,test_ds_x,test_ds_y=df_split(downsample_by_zero)\n",
    "\n",
    "lr_downsample=LogisticRegression()\n",
    "lr_model_downsample=lr_downsample.fit(train_ds_x,train_ds_y)\n",
    "pred_ds_lr=lr_model_downsample.predict(test_ds_x)\n",
    "\n",
    "print_measures('Logistic Regression with downsampling','Logistic Regression','Yes','No',test_ds_y,pred_ds_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSVC on Actual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy by Linear_SVC without downsampling Model is : 0.963581 \n",
      "The confusion matrix for the model is : \n",
      " [[401475      0]\n",
      " [ 15174      0]]\n",
      "The F1 score for this model is :  0.0\n",
      "The Precision score for this model is :  0.0\n",
      "The Recall score for this model is :  0.0\n"
     ]
    }
   ],
   "source": [
    "lsvc=LinearSVC()\n",
    "lsvc_model=lsvc.fit(train_x,train_y)\n",
    "pred_lsvc_y=lsvc_model.predict(test_x)\n",
    "print_measures('Linear_SVC without downsampling','Linear_SVC','No','No',test_y,pred_lsvc_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSVC on Downsampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy by Linear_SVC with downsampling Model is : 0.511655 \n",
      "The confusion matrix for the model is : \n",
      " [[14865   337]\n",
      " [14495   675]]\n",
      "The F1 score for this model is :  0.08342602892102335\n",
      "The Precision score for this model is :  0.6669960474308301\n",
      "The Recall score for this model is :  0.044495715227422544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lsvc_ds=LinearSVC()\n",
    "lsvc_model_ds=lsvc.fit(train_ds_x,train_ds_y)\n",
    "pred_lsvcds_y=lsvc_model.predict(test_ds_x)\n",
    "print_measures('Linear_SVC with downsampling','Linear_SVC','Yes','No',test_ds_y,pred_lsvcds_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSVC on Actual Data with Hyperparmeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] max_iter=100 ....................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ max_iter=100, score=0.963, total=   7.6s\n",
      "[CV] max_iter=100 ....................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   15.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ max_iter=100, score=0.963, total=   7.6s\n",
      "[CV] max_iter=100 ....................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ max_iter=100, score=0.963, total=   8.0s\n",
      "[CV] max_iter=500 ....................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ max_iter=500, score=0.963, total=  34.7s\n",
      "[CV] max_iter=500 ....................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ max_iter=500, score=0.963, total=  34.2s\n",
      "[CV] max_iter=500 ....................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ max_iter=500, score=0.963, total=  33.2s\n",
      "[CV] max_iter=1000 ...................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... max_iter=1000, score=0.963, total= 1.1min\n",
      "[CV] max_iter=1000 ...................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... max_iter=1000, score=0.963, total= 1.4min\n",
      "[CV] max_iter=1000 ...................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... max_iter=1000, score=0.963, total= 1.3min\n",
      "{'max_iter': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "grid_param={'max_iter':[True,False],'max_iter':[100,500,1000]}\n",
    "grid=GridSearchCV(LinearSVC(),grid_param,verbose=3)\n",
    "grid.fit(train_x,train_y)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSVC on Actual Data with best choosen hperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy by Linear_SVC without downsampling with Parameters Model is : 0.963581 \n",
      "The confusion matrix for the model is : \n",
      " [[401475      0]\n",
      " [ 15174      0]]\n",
      "The F1 score for this model is :  0.0\n",
      "The Precision score for this model is :  0.0\n",
      "The Recall score for this model is :  0.0\n"
     ]
    }
   ],
   "source": [
    "lsvc=LinearSVC(max_iter=500)\n",
    "lsvcP_model=lsvc.fit(train_x,train_y)\n",
    "pred_lsvcP_y=lsvcP_model.predict(test_x)\n",
    "print_measures('Linear_SVC without downsampling with Parameters','Linear_SVC_Hyperparameterize','No','No',test_y,pred_lsvcP_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC on 100K record with Feature Optimatization and Imbalance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos=df[['id','target','ps_ind_01', 'ps_ind_03', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15', 'ps_ind_16_bin', 'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01', 'ps_reg_02', 'ps_car_02_cat', 'ps_car_04_cat', 'ps_car_06_cat', 'ps_car_08_cat', 'ps_car_10_cat', 'ps_car_11_cat', 'ps_car_13', 'ps_car_15', 'ps_calc_01', 'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06', 'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11', 'ps_calc_12', 'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin', 'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin','ps_calc_19_bin','ps_calc_20_bin']]\n",
    "X = df_pos.iloc[:,2:] \n",
    "y = df_pos.iloc[:,1]   \n",
    "\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=25)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score'] \n",
    "feature_optimization=list(featureScores.nlargest(12,'Score')['Specs'])\n",
    "feature_optimization.append('target')\n",
    "feature_optimization.append('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\pandas\\core\\frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy by SVC with Imbalance check, feature optimization and 100k records Model is : 0.754429 \n",
      "The confusion matrix for the model is : \n",
      " [[52144 15257]\n",
      " [ 1933   666]]\n",
      "The F1 score for this model is :  0.07191448007774538\n",
      "The Precision score for this model is :  0.041826289015888966\n",
      "The Recall score for this model is :  0.2562524047710658\n"
     ]
    }
   ],
   "source": [
    "df_100k=df[feature_optimization].sample(n=100000,random_state=7,replace=False)\n",
    "train_svc_x,train_svc_y,test_svc_x,test_svc_y=df_split(df_100k)\n",
    "svc= SVC(class_weight ='balanced')\n",
    "\n",
    "svc_model=svc.fit(train_svc_x,train_svc_y)\n",
    "pred_svc_y=svc_model.predict(test_svc_x)\n",
    "print_measures('SVC with Imbalance check, feature optimization and 100k records','Linear_SVC_100k_Feature_Optimization','No','No',test_svc_y,pred_svc_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST on Actual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy by XGBoost on Actual Data Model is : 0.963578 \n",
      "The confusion matrix for the model is : \n",
      " [[401474      1]\n",
      " [ 15174      0]]\n",
      "The F1 score for this model is :  0.0\n",
      "The Precision score for this model is :  0.0\n",
      "The Recall score for this model is :  0.0\n"
     ]
    }
   ],
   "source": [
    "xg=XGBClassifier()\n",
    "xg_model=xg.fit(train_x,train_y)\n",
    "pred_xg=xg_model.predict(test_x)\n",
    "print_measures('XGBoost on Actual Data','XGBoost','No','No',test_y,pred_xg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST on Downsample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy by XGBoost with Downsampled Data Model is : 0.588964 \n",
      "The confusion matrix for the model is : \n",
      " [[9161 6041]\n",
      " [6443 8727]]\n",
      "The F1 score for this model is :  0.5830048767452736\n",
      "The Precision score for this model is :  0.5909398699891658\n",
      "The Recall score for this model is :  0.5752801582069875\n"
     ]
    }
   ],
   "source": [
    "xg_ds=XGBClassifier()\n",
    "xg_model_ds=xg_ds.fit(train_ds_x,train_ds_y)\n",
    "pred_xg_ds=xg_model_ds.predict(test_ds_x)\n",
    "print_measures('XGBoost with Downsampled Data','XGBoost','Yes','No',test_ds_y,pred_xg_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onehot encoder on Actual Data with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\pandas\\core\\frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy by One hot Encoder on Actual Data with XGBoost Model is : 0.963581 \n",
      "The confusion matrix for the model is : \n",
      " [[401475      0]\n",
      " [ 15174      0]]\n",
      "The F1 score for this model is :  0.0\n",
      "The Precision score for this model is :  0.0\n",
      "The Recall score for this model is :  0.0\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = ['ps_reg_01', 'ps_reg_02', 'ps_calc_01', 'ps_calc_02', 'ps_calc_03','ps_ind_02_cat','ps_ind_01', 'ps_ind_03', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_ind_14', 'ps_ind_15', 'ps_car_01_cat', 'ps_car_03_cat', 'ps_car_04_cat', 'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_09_cat', 'ps_car_10_cat', 'ps_car_11_cat', 'ps_car_11', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06', 'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11', 'ps_calc_12', 'ps_calc_13', 'ps_calc_14']\n",
    "\n",
    "def convert_ohe_df(dataframe):\n",
    "    my_result = pd.DataFrame()\n",
    "    temp = pd.DataFrame()\n",
    "    for runner in categorical_columns:\n",
    "        temp = pd.get_dummies(dataframe[runner], prefix=runner)\n",
    "        my_result[temp.columns] = temp\n",
    "    return my_result\n",
    "\n",
    "temp_df=copy.deepcopy(df)\n",
    "ohe_df=convert_ohe_df(temp_df)\n",
    "temp_df.drop(categorical_columns,axis=1,inplace=True)\n",
    "result=pd.concat([temp_df,ohe_df],axis=1)\n",
    "\n",
    "train_ohe_x,train_ohe_y,test_ohe_x,test_ohe_y=df_split(result)\n",
    "xg_ohe=XGBClassifier()\n",
    "xg_ohe_model=xg_ohe.fit(train_ohe_x,train_ohe_y)\n",
    "pred_ohe_xg=xg_ohe_model.predict(test_ohe_x)\n",
    "print_measures('One hot Encoder on Actual Data with XGBoost','XGBoost_OHE','No','No',test_ohe_y,pred_ohe_xg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onehot encoder on DownSampled Data with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy by One hot Encoder on Actual Data with XGBoost Model is : 0.585671 \n",
      "The confusion matrix for the model is : \n",
      " [[9080 6122]\n",
      " [6462 8708]]\n",
      "The F1 score for this model is :  0.5805333333333333\n",
      "The Precision score for this model is :  0.5871881321645314\n",
      "The Recall score for this model is :  0.5740276862228082\n"
     ]
    }
   ],
   "source": [
    "ohe_ds_df=convert_ohe_df(downsample_by_zero)\n",
    "temp_ohe_ds_df=copy.deepcopy(downsample_by_zero)\n",
    "temp_ohe_ds_df.drop(categorical_columns,axis=1,inplace=True)\n",
    "result_ohe_ds=pd.concat([temp_ohe_ds_df,ohe_ds_df],axis=1)\n",
    "\n",
    "train_oheds_x,train_oheds_y,test_oheds_x,test_oheds_y=df_split(result_ohe_ds)\n",
    "xg_ds_ohe=XGBClassifier()\n",
    "xg_ohe_ds_model=xg_ds_ohe.fit(train_oheds_x,train_oheds_y)\n",
    "pred_ohe_ds_xg=xg_ohe_ds_model.predict(test_oheds_x)\n",
    "print_measures('One hot Encoder on Actual Data with XGBoost','XGBoost_OHE','Yes','No',test_oheds_y,pred_ohe_ds_xg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBOOST on Actual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy by AdaBoost on Actual Data Model is : 0.963578 \n",
      "The confusion matrix for the model is : \n",
      " [[401474      1]\n",
      " [ 15174      0]]\n",
      "The F1 score for this model is :  0.0\n",
      "The Precision score for this model is :  0.0\n",
      "The Recall score for this model is :  0.0\n"
     ]
    }
   ],
   "source": [
    "adab=AdaBoostClassifier()\n",
    "adab_model=adab.fit(train_x,train_y)\n",
    "pred_adab=adab_model.predict(test_x)\n",
    "print_measures('AdaBoost on Actual Data','AdaBoost','No','NO',test_y,pred_adab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBOOST on Downsampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy by AdaBoost on Downsampled Data Model is : 0.583761 \n",
      "The confusion matrix for the model is : \n",
      " [[9171 6031]\n",
      " [6611 8559]]\n",
      "The F1 score for this model is :  0.5752016129032257\n",
      "The Precision score for this model is :  0.5866346812885538\n",
      "The Recall score for this model is :  0.5642056690837178\n"
     ]
    }
   ],
   "source": [
    "adab_ds=AdaBoostClassifier()\n",
    "adab_model_ds=adab_ds.fit(train_ds_x,train_ds_y)\n",
    "pred_adab_ds=adab_model_ds.predict(test_ds_x)\n",
    "print_measures('AdaBoost on Downsampled Data','AdaBoost','Yes','NO',test_ds_y,pred_adab_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 512)               29696     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 194,178\n",
      "Trainable params: 194,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 142850 samples, validate on 35713 samples\n",
      "Epoch 1/10\n",
      "142850/142850 [==============================] - 16s 110us/step - loss: 0.1610 - acc: 0.9629 - val_loss: 0.1499 - val_acc: 0.9654\n",
      "Epoch 2/10\n",
      "142850/142850 [==============================] - 13s 88us/step - loss: 0.1580 - acc: 0.9630 - val_loss: 0.1490 - val_acc: 0.9654\n",
      "Epoch 3/10\n",
      "142850/142850 [==============================] - 14s 98us/step - loss: 0.1575 - acc: 0.9630 - val_loss: 0.1681 - val_acc: 0.9654\n",
      "Epoch 4/10\n",
      "142850/142850 [==============================] - 13s 93us/step - loss: 0.1588 - acc: 0.9630 - val_loss: 0.1499 - val_acc: 0.9654\n",
      "Epoch 5/10\n",
      "142850/142850 [==============================] - 13s 90us/step - loss: 0.1573 - acc: 0.9630 - val_loss: 0.1499 - val_acc: 0.9654\n",
      "Epoch 6/10\n",
      "142850/142850 [==============================] - 13s 88us/step - loss: 0.1579 - acc: 0.9630 - val_loss: 0.1490 - val_acc: 0.9654\n",
      "Epoch 7/10\n",
      "142850/142850 [==============================] - 13s 90us/step - loss: 0.1569 - acc: 0.9630 - val_loss: 0.1524 - val_acc: 0.9654\n",
      "Epoch 8/10\n",
      "142850/142850 [==============================] - 14s 98us/step - loss: 0.1576 - acc: 0.9630 - val_loss: 0.1519 - val_acc: 0.9654\n",
      "Epoch 9/10\n",
      "142850/142850 [==============================] - 13s 88us/step - loss: 0.1570 - acc: 0.9630 - val_loss: 0.1504 - val_acc: 0.9654\n",
      "Epoch 10/10\n",
      "142850/142850 [==============================] - 12s 88us/step - loss: 0.1568 - acc: 0.9630 - val_loss: 0.1512 - val_acc: 0.9654\n",
      "416649/416649 [==============================] - 28s 67us/step\n",
      "Test Score 0.15749750169542098\n",
      "Test Accuracy 0.9635808558282871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy by MLP on Actual Data Model is : 0.963581 \n",
      "The confusion matrix for the model is : \n",
      " [[401475      0]\n",
      " [ 15174      0]]\n",
      "The F1 score for this model is :  0.0\n",
      "The Precision score for this model is :  0.0\n",
      "The Recall score for this model is :  0.0\n"
     ]
    }
   ],
   "source": [
    "NB_EPOCH=10\n",
    "BATCH_SIZE=128\n",
    "VERBOSE=1\n",
    "OPTIMIZER=Adam()\n",
    "VALIDATION_SPLIT=0.2\n",
    "\n",
    "train_mlp_y=pd.get_dummies(train_y, prefix='target')\n",
    "train_mlp_Y=np.array(train_mlp_y)\n",
    "train_mlp_Y=train_mlp_Y.reshape(-1,2)\n",
    "\n",
    "test_mlp_y=pd.get_dummies(test_y, prefix='target')\n",
    "test_mlp_Y=np.array(test_mlp_y)\n",
    "test_mlp_Y=test_mlp_Y.reshape(-1,2)\n",
    "\n",
    "train_mlp_x=np.array(train_x)\n",
    "test_mlp_x=np.array(test_x)\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Dense(512, input_dim=57, activation='tanh'))\n",
    "# model.add(Activation('tanh'))\n",
    "model.add(Dense(256,activation='tanh'))\n",
    "# model.add(Activation('tanh'))\n",
    "model.add(Dense(128,activation='tanh'))\n",
    "# model.add(Activation('tanh'))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "# model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "history=model.fit(train_mlp_x,train_mlp_y,batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score=model.evaluate(test_mlp_x,test_mlp_y,verbose=VERBOSE)\n",
    "\n",
    "print('Test Score',score[0])\n",
    "print('Test Accuracy',score[1])\n",
    "\n",
    "pred_mlp=model.predict(test_mlp_x)\n",
    "test_mlp_Y=np.array(test_y)\n",
    "test_mlp_Y=test_mlp_Y.reshape(-1,1)\n",
    "z=pred_mlp.argmax(axis=1)\n",
    "metrics.accuracy_score(test_mlp_Y, z)\n",
    "print_measures('MLP on Actual Data','MLP','No','No',test_mlp_Y,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with Normalize Actual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_normalise(feature):\n",
    "    mu=np.mean(feature,axis=0)\n",
    "    sigma=np.std(feature,axis=0)\n",
    "    return (feature-mu)/sigma\n",
    "\n",
    "# X=feature_normalise(X)\n",
    "train_norm=feature_normalise(train_x)\n",
    "train_mlp_x=np.array(train_norm)\n",
    "test_norm=feature_normalise(test_x)\n",
    "test_mlp_x=np.array(test_norm)\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler()\n",
    "# X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 512)               29696     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 194,178\n",
      "Trainable params: 194,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 142850 samples, validate on 35713 samples\n",
      "Epoch 1/10\n",
      "142850/142850 [==============================] - 14s 100us/step - loss: 0.1691 - acc: 0.9592 - val_loss: 0.1520 - val_acc: 0.9654\n",
      "Epoch 2/10\n",
      "142850/142850 [==============================] - 14s 95us/step - loss: 0.1565 - acc: 0.9630 - val_loss: 0.1498 - val_acc: 0.9654\n",
      "Epoch 3/10\n",
      "142850/142850 [==============================] - 13s 94us/step - loss: 0.1553 - acc: 0.9630 - val_loss: 0.1496 - val_acc: 0.9653\n",
      "Epoch 4/10\n",
      "142850/142850 [==============================] - 13s 92us/step - loss: 0.1547 - acc: 0.9630 - val_loss: 0.1499 - val_acc: 0.9653\n",
      "Epoch 5/10\n",
      "142850/142850 [==============================] - 12s 86us/step - loss: 0.1539 - acc: 0.9630 - val_loss: 0.1496 - val_acc: 0.9654\n",
      "Epoch 6/10\n",
      "142850/142850 [==============================] - 13s 93us/step - loss: 0.1530 - acc: 0.9630 - val_loss: 0.1520 - val_acc: 0.9653\n",
      "Epoch 7/10\n",
      "142850/142850 [==============================] - 13s 93us/step - loss: 0.1518 - acc: 0.9630 - val_loss: 0.1497 - val_acc: 0.9653\n",
      "Epoch 8/10\n",
      "142850/142850 [==============================] - 13s 94us/step - loss: 0.1499 - acc: 0.9630 - val_loss: 0.1524 - val_acc: 0.9653\n",
      "Epoch 9/10\n",
      "142850/142850 [==============================] - 13s 89us/step - loss: 0.1465 - acc: 0.9630 - val_loss: 0.1544 - val_acc: 0.9653\n",
      "Epoch 10/10\n",
      "142850/142850 [==============================] - 12s 87us/step - loss: 0.1427 - acc: 0.9630 - val_loss: 0.1573 - val_acc: 0.9653\n",
      "416649/416649 [==============================] - 28s 67us/step\n",
      "Test Score 0.1627494854444473\n",
      "Test Accuracy 0.9635520546071153\n",
      "The accuracy by MLP with Normalization Model is : 0.963552 \n",
      "The confusion matrix for the model is : \n",
      " [[401458     17]\n",
      " [ 15169      5]]\n",
      "The F1 score for this model is :  0.0006580679126085812\n",
      "The Precision score for this model is :  0.22727272727272727\n",
      "The Recall score for this model is :  0.0003295110056675893\n"
     ]
    }
   ],
   "source": [
    "NB_EPOCH=10\n",
    "BATCH_SIZE=128\n",
    "VERBOSE=1\n",
    "OPTIMIZER=Adam()\n",
    "VALIDATION_SPLIT=0.2\n",
    "\n",
    "train_mlp_y=pd.get_dummies(train_y, prefix='target')\n",
    "train_mlp_Y=np.array(train_mlp_y)\n",
    "train_mlp_Y=train_mlp_Y.reshape(-1,2)\n",
    "\n",
    "test_mlp_y=pd.get_dummies(test_y, prefix='target')\n",
    "test_mlp_Y=np.array(test_mlp_y)\n",
    "test_mlp_Y=test_mlp_Y.reshape(-1,2)\n",
    "\n",
    "# train_mlp_x=np.array(train_x)\n",
    "# test_mlp_x=np.array(test_x)\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Dense(512, input_dim=57, activation='tanh'))\n",
    "# model.add(Activation('tanh'))\n",
    "model.add(Dense(256,activation='tanh'))\n",
    "# model.add(Activation('tanh'))\n",
    "model.add(Dense(128,activation='tanh'))\n",
    "# model.add(Activation('tanh'))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "# model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "history=model.fit(train_mlp_x,train_mlp_y,batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score=model.evaluate(test_mlp_x,test_mlp_y,verbose=VERBOSE)\n",
    "\n",
    "print('Test Score',score[0])\n",
    "print('Test Accuracy',score[1])\n",
    "\n",
    "\n",
    "test_mlp_Y=np.array(test_y)\n",
    "pred_mlp=model.predict(test_mlp_x)\n",
    "test_mlp_Y=test_mlp_Y.reshape(-1,1)\n",
    "z=pred_mlp.argmax(axis=1)\n",
    "metrics.accuracy_score(test_mlp_Y, z)\n",
    "print_measures('MLP with Normalization','MLP','NO','Yes',test_mlp_Y,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with Downsampled Actual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 512)               29696     \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 194,178\n",
      "Trainable params: 194,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10412 samples, validate on 2604 samples\n",
      "Epoch 1/10\n",
      "10412/10412 [==============================] - 2s 223us/step - loss: 1.0292 - acc: 0.5023 - val_loss: 0.7580 - val_acc: 0.5111\n",
      "Epoch 2/10\n",
      "10412/10412 [==============================] - 0s 39us/step - loss: 0.7235 - acc: 0.4990 - val_loss: 0.7159 - val_acc: 0.5115\n",
      "Epoch 3/10\n",
      "10412/10412 [==============================] - 0s 46us/step - loss: 0.7057 - acc: 0.5010 - val_loss: 0.6956 - val_acc: 0.5004\n",
      "Epoch 4/10\n",
      "10412/10412 [==============================] - 1s 50us/step - loss: 0.6925 - acc: 0.5114 - val_loss: 0.6930 - val_acc: 0.5019\n",
      "Epoch 5/10\n",
      "10412/10412 [==============================] - 0s 43us/step - loss: 0.6893 - acc: 0.5233 - val_loss: 0.6902 - val_acc: 0.5234\n",
      "Epoch 6/10\n",
      "10412/10412 [==============================] - 0s 41us/step - loss: 0.6872 - acc: 0.5433 - val_loss: 0.6914 - val_acc: 0.5127\n",
      "Epoch 7/10\n",
      "10412/10412 [==============================] - 0s 45us/step - loss: 0.6871 - acc: 0.5350 - val_loss: 0.6910 - val_acc: 0.5330\n",
      "Epoch 8/10\n",
      "10412/10412 [==============================] - 0s 41us/step - loss: 0.6841 - acc: 0.5497 - val_loss: 0.6880 - val_acc: 0.5384\n",
      "Epoch 9/10\n",
      "10412/10412 [==============================] - 0s 39us/step - loss: 0.6824 - acc: 0.5572 - val_loss: 0.6872 - val_acc: 0.5480\n",
      "Epoch 10/10\n",
      "10412/10412 [==============================] - 0s 41us/step - loss: 0.6824 - acc: 0.5505 - val_loss: 0.6872 - val_acc: 0.5472\n",
      "30372/30372 [==============================] - ETA:  - 2s 72us/step\n",
      "Test Score 0.6858716323789237\n",
      "Test Accuracy 0.5464901883313579\n",
      "The accuracy by MLP with Normalization Model is : 0.546490 \n",
      "The confusion matrix for the model is : \n",
      " [[ 5549  9653]\n",
      " [ 4121 11049]]\n",
      "The F1 score for this model is :  0.6160236396074933\n",
      "The Precision score for this model is :  0.5337165491256883\n",
      "The Recall score for this model is :  0.728345418589321\n"
     ]
    }
   ],
   "source": [
    "NB_EPOCH=10\n",
    "BATCH_SIZE=1000\n",
    "VERBOSE=1\n",
    "OPTIMIZER=Adam()\n",
    "VALIDATION_SPLIT=0.2\n",
    "\n",
    "train_mlp_y=pd.get_dummies(train_ds_y, prefix='target')\n",
    "train_mlp_Y=np.array(train_mlp_y)\n",
    "train_mlp_Y=train_mlp_Y.reshape(-1,2)\n",
    "\n",
    "test_mlp_y=pd.get_dummies(test_ds_y, prefix='target')\n",
    "test_mlp_Y=np.array(test_mlp_y)\n",
    "test_mlp_Y=test_mlp_Y.reshape(-1,2)\n",
    "\n",
    "train_mlp_x=np.array(train_ds_x)\n",
    "test_mlp_x=np.array(test_ds_x)\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Dense(512, input_dim=57, activation='tanh'))\n",
    "# model.add(Activation('tanh'))\n",
    "model.add(Dense(256,activation='tanh'))\n",
    "# model.add(Activation('tanh'))\n",
    "model.add(Dense(128,activation='tanh'))\n",
    "# model.add(Activation('tanh'))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "# model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "history=model.fit(train_mlp_x,train_mlp_y,batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score=model.evaluate(test_mlp_x,test_mlp_y,verbose=VERBOSE)\n",
    "\n",
    "print('Test Score',score[0])\n",
    "print('Test Accuracy',score[1])\n",
    "\n",
    "\n",
    "test_mlp_Y=np.array(test_ds_y)\n",
    "pred_mlp=model.predict(test_mlp_x)\n",
    "test_mlp_Y=test_mlp_Y.reshape(-1,1)\n",
    "z=pred_mlp.argmax(axis=1)\n",
    "metrics.accuracy_score(test_mlp_Y, z)\n",
    "print_measures('MLP with Normalization','MLP','Yes','No',test_mlp_Y,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>DownSample</th>\n",
       "      <th>Normalise</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.963581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>0.584025</td>\n",
       "      <td>0.589020</td>\n",
       "      <td>0.553065</td>\n",
       "      <td>0.570477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linear</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.963581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linear_SVC</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.963581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Linear_SVC</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>0.511655</td>\n",
       "      <td>0.666996</td>\n",
       "      <td>0.044496</td>\n",
       "      <td>0.083426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Linear_SVC_Hyperparameterize</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.963581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Linear_SVC_100k_Feature_Optimization</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.754429</td>\n",
       "      <td>0.041826</td>\n",
       "      <td>0.256252</td>\n",
       "      <td>0.071914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.963578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>0.588964</td>\n",
       "      <td>0.590940</td>\n",
       "      <td>0.575280</td>\n",
       "      <td>0.583005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBoost_OHE</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.963581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBoost_OHE</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>0.585671</td>\n",
       "      <td>0.587188</td>\n",
       "      <td>0.574028</td>\n",
       "      <td>0.580533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>No</td>\n",
       "      <td>NO</td>\n",
       "      <td>0.963578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "      <td>0.583761</td>\n",
       "      <td>0.586635</td>\n",
       "      <td>0.564206</td>\n",
       "      <td>0.575202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MLP</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.963581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MLP</td>\n",
       "      <td>NO</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.963552</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>0.546490</td>\n",
       "      <td>0.533717</td>\n",
       "      <td>0.728345</td>\n",
       "      <td>0.616024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Model DownSample Normalise  Accuracy  \\\n",
       "0                    Logistic Regression         No        No  0.963581   \n",
       "1                    Logistic Regression        Yes        No  0.584025   \n",
       "2                                 Linear         No        No  0.963581   \n",
       "3                             Linear_SVC         No        No  0.963581   \n",
       "4                             Linear_SVC        Yes        No  0.511655   \n",
       "5           Linear_SVC_Hyperparameterize         No        No  0.963581   \n",
       "6   Linear_SVC_100k_Feature_Optimization         No        No  0.754429   \n",
       "7                                XGBoost         No        No  0.963578   \n",
       "8                                XGBoost        Yes        No  0.588964   \n",
       "9                            XGBoost_OHE         No        No  0.963581   \n",
       "10                           XGBoost_OHE        Yes        No  0.585671   \n",
       "11                              AdaBoost         No        NO  0.963578   \n",
       "12                              AdaBoost        Yes        NO  0.583761   \n",
       "13                                   MLP         No        No  0.963581   \n",
       "14                                   MLP         NO       Yes  0.963552   \n",
       "15                                   MLP        Yes        No  0.546490   \n",
       "\n",
       "    Precision    Recall  F1-Score  \n",
       "0    0.000000  0.000000  0.000000  \n",
       "1    0.589020  0.553065  0.570477  \n",
       "2    0.000000  0.000000  0.000000  \n",
       "3    0.000000  0.000000  0.000000  \n",
       "4    0.666996  0.044496  0.083426  \n",
       "5    0.000000  0.000000  0.000000  \n",
       "6    0.041826  0.256252  0.071914  \n",
       "7    0.000000  0.000000  0.000000  \n",
       "8    0.590940  0.575280  0.583005  \n",
       "9    0.000000  0.000000  0.000000  \n",
       "10   0.587188  0.574028  0.580533  \n",
       "11   0.000000  0.000000  0.000000  \n",
       "12   0.586635  0.564206  0.575202  \n",
       "13   0.000000  0.000000  0.000000  \n",
       "14   0.227273  0.000330  0.000658  \n",
       "15   0.533717  0.728345  0.616024  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
